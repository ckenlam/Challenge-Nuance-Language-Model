{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the \"hound-train.txt\" provided with this challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hound-train.txt', encoding=\"utf-8\") as f:\n",
    "    train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " \"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\\n\",\n",
       " '\\n',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with\\n',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or\\n',\n",
       " 're-use it under the terms of the Project Gutenberg License included\\n',\n",
       " 'with this eBook or online at www.gutenberg.net\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Title: The Adventures of Sherlock Holmes\\n',\n",
       " '\\n',\n",
       " 'Author: Arthur Conan Doyle\\n',\n",
       " '\\n',\n",
       " 'Release Date: November 29, 2002 [EBook #1661]\\n',\n",
       " 'Last Updated: May 20, 2019\\n',\n",
       " '\\n',\n",
       " 'Language: English\\n',\n",
       " '\\n',\n",
       " 'Character set encoding: UTF-8\\n',\n",
       " '\\n',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Produced by an anonymous Project Gutenberg volunteer and Jose Menendez\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'cover\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The Adventures of Sherlock Holmes\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'by Arthur Conan Doyle\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Contents\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   I.     A Scandal in Bohemia\\n',\n",
       " '   II.    The Red-Headed League\\n',\n",
       " '   III.   A Case of Identity\\n',\n",
       " '   IV.    The Boscombe Valley Mystery\\n',\n",
       " '   V.     The Five Orange Pips\\n',\n",
       " '   VI.    The Man with the Twisted Lip\\n',\n",
       " '   VII.   The Adventure of the Blue Carbuncle\\n',\n",
       " '   VIII.  The Adventure of the Speckled Band\\n',\n",
       " '   IX.    The Adventure of the Engineer’s Thumb\\n',\n",
       " '   X.     The Adventure of the Noble Bachelor\\n',\n",
       " '   XI.    The Adventure of the Beryl Coronet\\n',\n",
       " '   XII.   The Adventure of the Copper Beeches\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'I. A SCANDAL IN BOHEMIA\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'I.\\n',\n",
       " '\\n',\n",
       " 'To Sherlock Holmes she is always _the_ woman. I have seldom heard him\\n',\n",
       " 'mention her under any other name. In his eyes she eclipses and\\n',\n",
       " 'predominates the whole of her sex. It was not that he felt any emotion\\n',\n",
       " 'akin to love for Irene Adler. All emotions, and that one particularly,\\n',\n",
       " 'were abhorrent to his cold, precise but admirably balanced mind. He\\n',\n",
       " 'was, I take it, the most perfect reasoning and observing machine that\\n',\n",
       " 'the world has seen, but as a lover he would have placed himself in a\\n',\n",
       " 'false position. He never spoke of the softer passions, save with a gibe\\n',\n",
       " 'and a sneer. They were admirable things for the observer—excellent for\\n',\n",
       " 'drawing the veil from men’s motives and actions. But for the trained\\n',\n",
       " 'reasoner to admit such intrusions into his own delicate and finely\\n',\n",
       " 'adjusted temperament was to introduce a distracting factor which might\\n',\n",
       " 'throw a doubt upon all his mental results. Grit in a sensitive\\n',\n",
       " 'instrument, or a crack in one of his own high-power lenses, would not\\n',\n",
       " 'be more disturbing than a strong emotion in a nature such as his. And\\n',\n",
       " 'yet there was but one woman to him, and that woman was the late Irene\\n',\n",
       " 'Adler, of dubious and questionable memory.\\n',\n",
       " '\\n',\n",
       " 'I had seen little of Holmes lately. My marriage had drifted us away\\n',\n",
       " 'from each other. My own complete happiness, and the home-centred\\n',\n",
       " 'interests which rise up around the man who first finds himself master\\n',\n",
       " 'of his own establishment, were sufficient to absorb all my attention,\\n',\n",
       " 'while Holmes, who loathed every form of society with his whole Bohemian\\n',\n",
       " 'soul, remained in our lodgings in Baker Street, buried among his old\\n',\n",
       " 'books, and alternating from week to week between cocaine and ambition,\\n',\n",
       " 'the drowsiness of the drug, and the fierce energy of his own keen\\n',\n",
       " 'nature. He was still, as ever, deeply attracted by the study of crime,\\n',\n",
       " 'and occupied his immense faculties and extraordinary powers of\\n',\n",
       " 'observation in following out those clues, and clearing up those\\n',\n",
       " 'mysteries which had been abandoned as hopeless by the official police.\\n',\n",
       " 'From time to time I heard some vague account of his doings: of his\\n',\n",
       " 'summons to Odessa in the case of the Trepoff murder, of his clearing up\\n',\n",
       " 'of the singular tragedy of the Atkinson brothers at Trincomalee, and\\n',\n",
       " 'finally of the mission which he had accomplished so delicately and\\n',\n",
       " 'successfully for the reigning family of Holland. Beyond these signs of\\n',\n",
       " 'his activity, however, which I merely shared with all the readers of\\n',\n",
       " 'the daily press, I knew little of my former friend and companion.\\n']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will only be using the content from the dataset that are relevant to the training of the language model by finding the starting and ending indexes containing the first sentence and the last sentence of the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "startString = 'To Sherlock Holmes she is always'\n",
    "startIndex = [i for i, s in enumerate(train_data) if startString in s][0]\n",
    "endString = 'Walsall, where I believe that she has met with considerable success.'\n",
    "endtIndex = [i for i, s in enumerate(train_data) if endString in s][0]\n",
    "train_data = ' '.join(train_data[startIndex:endtIndex+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To Sherlock Holmes she is always _the_ woman. I have seldom heard him\\n mention her under any other name. In his eyes she eclipses and\\n predominates the whole of her sex. It was not that he felt any emotion\\n akin to love for Irene Adler. All emotions, and that one particularly,\\n were abhorrent to his cold, precise but admirably balanced mind. He\\n was, I take it, the most perfect reasoning and observing machine that\\n the world has seen, but as a lover he would have placed himself in a\\n false position. He never spoke of the softer passions, save with a gibe\\n and a sneer. They were admirable things for the observer—excellent for\\n drawing the veil from men’s motives and actions. But for the trained\\n reasoner to admit such intrusions into his own delicate and finely\\n adjusted temperament was to introduce a distracting factor which might\\n throw a doubt upon all his mental results. Grit in a sensitive\\n instrument, or a crack in one of his own high-power lenses, would not\\n be more disturbing than a strong emotion in a nature such as his. And\\n yet there was but one woman to him, and that woman was the late Irene\\n Adler, of dubious and questionable memory.\\n \\n I had seen little of Holmes lately. My marriage had drifted us away\\n from each other. My own complete happiness, and the home-centred\\n interests which rise up around the man who first finds himself master\\n of his own establishment, were sufficient to absorb all my attention,\\n while Holmes, who loathed every form of society with his whole Bohemian\\n soul, remained in our lodgings in Baker Street, buried among his old\\n books, and alternating from week to week between cocaine and ambition,\\n the drowsiness of the drug, and the fierce energy of his own keen\\n nature. He was still, as ever, deeply attracted by the study of crime,\\n and occupied his immense faculties and extraordinary powers of\\n observation in following out those clues, and clearing up those\\n mysteries which had been abandoned as hopeless by the official police.\\n From time to time I heard some vague account of his doings: of his\\n summons to Odessa in the case of the Trepoff murder, of his clearing up\\n of the singular tragedy of the Atkinson brothers at Trincomalee, and\\n finally of the mission which he had accomplished so delicately and\\n successfully for the reigning family of Holland. Beyond these signs of\\n his activity, however, which I merely shared with all the readers of\\n the daily press, I knew little of my former friend and companion.\\n \\n One night—it was on the twentieth of March, 1888—I was returning from a\\n journey to a patient (for I had now returned to civil practice), when\\n my way led me through Baker Street. As I passed the well-remembered\\n door, which must always be associated in my mind with my wooing, and\\n with the dark incidents of the Study in Scarlet, I was seized with a\\n keen desire to see Holmes again, and to know how he was employing his\\n extraordinary powers. His rooms were brilliantly lit, and, even as I\\n looked up, I saw his tall, spare figure pass twice in a dark silhouette\\n against the blind. He was pacing the room swiftly, eagerly, with his\\n head sunk upon his chest and his hands clasped behind him. To me, who\\n knew his every mood and habit, his attitude and manner told their own\\n story. He was at work again. He had risen out of his drug-created\\n dreams and was hot upon the scent of some new problem. I rang the bell\\n and was shown up to the chamber which had formerly been in part my own.\\n \\n His manner was not effusive. It seldom was; but he was glad, I think,\\n to see me. With hardly a word spoken, but with a kindly eye, he waved\\n me to an armchair, threw across his case of cigars, and indicated a\\n spirit case and a gasogene in the corner. Then he stood before the fire\\n and looked me over in his singular introspective fashion.\\n \\n “Wedlock suits you,” he remarked. “I think, Watson, that you have put\\n on seven and a half pounds since I saw you.”\\n \\n “Seven!” I answered.\\n \\n “Indeed, I should have thought a little more. Just a trifle more, I\\n fancy, Watson. And in practice again, I observe. You did not tell me\\n that you intended to go into harness.”\\n \\n “Then, how do you know?”\\n \\n “I see it, I deduce it. How do I know that you have been getting\\n yourself very wet lately, and that you have a most clumsy and careless\\n servant girl?”\\n \\n “My dear Holmes,” said I, “this is too much. You would certainly have\\n been burned, had you lived a few centuries ago. It is true that I had a\\n country walk on Thursday and came home in a dreadful mess, but as I\\n have changed my clothes I can’t imagine how you deduce it. As to Mary\\n Jane, she is incorrigible, and my wife has given her notice, but there,\\n again, I fail to see how you work it out.”\\n \\n He chuckled to himself and rubbed his long, nervous hands together.\\n \\n “It is simplicity itself,” said he; “my eyes tell me that on the inside\\n of your left shoe, just where the firelight strikes it, the leather is\\n scored by six almost parallel cuts. Obviously '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hound-test.txt', encoding=\"utf-8\") as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate realistic sentences, I need to preserve the punctuations in the dataset. However, Keras' Tokenizer will remove all punctuations by default. To get around that, I will have to add a space to any punctuations and remove the desired punctuations from the Tokenizer's filter. I will also capitalize all letters to match what is in the test set. \n",
    "\n",
    "I will then organize the tokens into sequences of 30 input words and 1 output word. That is, sequences of 31 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', oov_token='OOV', lower=False)\n",
    "\n",
    "def train_data_processing(text_data, seq_length=50):\n",
    "    #Capitalize all letters\n",
    "    text = text_data.upper()\n",
    "    #Isolate punctuations; except the periods\n",
    "    cleanup_dict = {\",\":\" ,\"\n",
    "               ,\"!\":\" !\"\n",
    "               ,\"?\":\" ?\"\n",
    "               ,\"\\n\":\"\"\n",
    "               ,\"_\":\"\"\n",
    "               ,\"“\":\"\"\n",
    "               ,\"”\":\"\"\n",
    "               ,\"(\":\"( \"\n",
    "               ,\")\":\" )\"\n",
    "               ,\"II.\":\"\"\n",
    "               ,\"III.\":\"\"\n",
    "               ,\"IV.\":\"\"\n",
    "               ,\"V.\":\"\"\n",
    "               ,\"VI.\":\"\" \n",
    "               ,\"VII.\":\"\"\n",
    "               ,\"VIII.\":\"\" \n",
    "               ,\"IX.\":\"\" \n",
    "               ,\"X.\":\"\"\n",
    "               ,\"XI.\":\"\" \n",
    "               ,\"XII.\":\"\"                    \n",
    "                }\n",
    "    for from_this, to_this in cleanup_dict.items():\n",
    "        text = text.replace(from_this, to_this)\n",
    "    #I want to preserve periods that designate accronyms (ones that are not followed by any space)\n",
    "    text = re.sub('\\.\\s+',' . ', text)\n",
    "    #Tokenize all words in the text\n",
    "    text = text_to_word_sequence(text, filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', lower=False)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    tokens = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    words_count = len(word_index)+1\n",
    "    #Turn the tokenized text into sequences of the specified length and append to the \"sequences\" list\n",
    "    sequences = []\n",
    "    length = seq_length +1\n",
    "    #For the first portion of words from position 0 to the chosen length\n",
    "    for i in range(1, length):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[0:i+1]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "    #For the rest of the data \n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "        \n",
    "    #Finding the maximum length in this dataset (just in case...)\n",
    "    max_sequence_length = max(len(x) for x in sequences)    \n",
    "    #Make sure all sequences are of the same length\n",
    "    sequences = pad_sequences(sequences,maxlen=max_sequence_length,padding='pre')\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "    return X,y,max_sequence_length,words_count, word_index\n",
    "\n",
    "def test_data_processing(text_data, tokenizer, seq_length=50):\n",
    "    text = text_data \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    #Tokenize all words in the text\n",
    "    text = text_to_word_sequence(text, filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', lower=False)\n",
    "    tokens = tokenizer.texts_to_sequences(text)\n",
    "    #Turn the tokenized text into sequences of the specified length and append to the \"sequences\" list\n",
    "    sequences = []\n",
    "    length = seq_length +1\n",
    "    \n",
    "    #For the first portion of words from position 0 to the chosen length\n",
    "    for i in range(1, length):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[0:i+1]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "    #For the rest of the data \n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "        \n",
    "    #Finding the maximum length in this dataset (just in case...)\n",
    "    max_sequence_length = max(len(x) for x in sequences)    \n",
    "    #Make sure all sequences are of the same length\n",
    "    sequences = pad_sequences(sequences,maxlen=length,padding='pre')\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decide on a sequence length\n",
    "sq_len = 30\n",
    "#Data Processing of the training set\n",
    "X, y, max_seq_length, total_words_count, word_index = train_data_processing(train_data,sq_len )\n",
    "#Data Processing of the test set\n",
    "Xtest, ytest = test_data_processing(test_data, tokenizer,sq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will combine  \"y\" and \"ytest\" to transform them into categorical data because this is essentially a (word) classification problem; after they are transformed, I will split them back again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mark the index to split the y-data after transformation\n",
    "split_index = len(y)\n",
    "#Combine the 2 y-data \n",
    "y_combined = np.concatenate((y, ytest))\n",
    "y_combined = ku.to_categorical(y_combined, num_classes=total_words_count)\n",
    "#Split the 2 sets\n",
    "y = y_combined[0:split_index]\n",
    "ytest = y_combined[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will directly use the test set as the validation set when training the model; that way, I will be able to observe the improvement on the perplexity measure at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = (Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, I will train a neural network with two Bi-directional LSTM hidden layers with 50 neurons each. I'm also including a Dropout layer to  prevent overfitting in the model. A dense fully connected layer with 100 neurons connects to the LSTM hidden layers. The model is validated on the perplexity metric at each epoch directly using the test set  as the validation set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used by Keras to report perplexity during training\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 30, 100)           819100    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 30, 100)           60400     \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8191)              827291    \n",
      "=================================================================\n",
      "Total params: 1,767,191\n",
      "Trainable params: 1,767,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words_count, 100, input_length=max_seq_length - 1))\n",
    "model.add(Dropout(rate=0.1))\n",
    "#model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.6, recurrent_dropout=0.6,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(50)))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(total_words_count, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[perplexity])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History \n",
    "from keras.callbacks import EarlyStopping\n",
    "batch_size = 50\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120454 samples, validate on 66729 samples\n",
      "Epoch 1/100\n",
      "120454/120454 [==============================] - 618s 5ms/step - loss: 6.0557 - perplexity: 529.7084 - val_loss: 6.0348 - val_perplexity: 472.1984\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ken.lam\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37_env\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with  val_perplexity available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120454/120454 [==============================] - 575s 5ms/step - loss: 5.5539 - perplexity: 279.8399 - val_loss: 5.9619 - val_perplexity: 467.2101\n",
      "Epoch 3/100\n",
      "120454/120454 [==============================] - 569s 5ms/step - loss: 5.3218 - perplexity: 220.4804 - val_loss: 5.9628 - val_perplexity: 501.9879\n",
      "Epoch 4/100\n",
      "120454/120454 [==============================] - 582s 5ms/step - loss: 5.1520 - perplexity: 185.9777 - val_loss: 5.9953 - val_perplexity: 562.9317\n",
      "Epoch 5/100\n",
      "120454/120454 [==============================] - 590s 5ms/step - loss: 5.0082 - perplexity: 161.7705 - val_loss: 6.0317 - val_perplexity: 620.4519\n",
      "Epoch 6/100\n",
      "120454/120454 [==============================] - 579s 5ms/step - loss: 4.8903 - perplexity: 142.9447 - val_loss: 6.1056 - val_perplexity: 729.8088\n",
      "Epoch 7/100\n",
      "120454/120454 [==============================] - 580s 5ms/step - loss: 4.7746 - perplexity: 127.1663 - val_loss: 6.1836 - val_perplexity: 834.9260\n",
      "Epoch 8/100\n",
      "120454/120454 [==============================] - 575s 5ms/step - loss: 4.6675 - perplexity: 113.1389 - val_loss: 6.2591 - val_perplexity: 967.2158\n",
      "Epoch 9/100\n",
      "120454/120454 [==============================] - 579s 5ms/step - loss: 4.5670 - perplexity: 102.2134 - val_loss: 6.3142 - val_perplexity: 1072.7974\n",
      "Epoch 10/100\n",
      "120454/120454 [==============================] - 575s 5ms/step - loss: 4.4771 - perplexity: 93.1877 - val_loss: 6.3857 - val_perplexity: 1245.2354\n",
      "Epoch 11/100\n",
      "120454/120454 [==============================] - 593s 5ms/step - loss: 4.3910 - perplexity: 85.2110 - val_loss: 6.4171 - val_perplexity: 1276.1859\n",
      "Epoch 12/100\n",
      "120454/120454 [==============================] - 589s 5ms/step - loss: 4.3087 - perplexity: 78.3013 - val_loss: 6.5028 - val_perplexity: 1462.8945\n",
      "Epoch 00012: early stopping\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# set earlystop and checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{perplexity:.2f}.h5\"\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=' val_perplexity', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [earlystop, checkpoint]\n",
    "\n",
    "#fit model to the data\n",
    "history = model.fit(X, y, epochs=epochs, verbose = 1\n",
    "                      , batch_size=batch_size\n",
    "                      , callbacks=callbacks_list\n",
    "                      ,validation_data=validation_set\n",
    "                     )\n",
    "print(\"Training completed!\")\n",
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5',compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity metric was defined earlier in the \"Defining the Model\" section. It is calculated by exponentiating the mean cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66729/66729 [==============================] - 105s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(Xtest, ytest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's loss is 6.502826203003522 and perplexity is 2192.33984375.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model's {model.metrics_names[0]} is {scores[0]} and {model.metrics_names[1]} is {scores[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My time was mainly spent on comparing the training set to the test set and coming up with data processing steps that would make the training set to ressemble the test set. The following is list of the data processing steps:\n",
    "- Capitalize all letters\n",
    "- Add space on both sides of the punctuation such as \"!\", \"?\", \",\", and \".\" (except for accronyms) to mimic the test set\n",
    "- Remove any punctuations that do not exist in the test set\n",
    "- Removing irrelevant content from the training set\n",
    "- Transform the training set into sequences of 31 words as input data to the LSTM Language Model\n",
    "\n",
    "*************** **update 2021 Jan 14th** ***************\n",
    "\n",
    "The following changes were made to the model:\n",
    "- Shortened the sequence length from 51 to 31\n",
    "- Added dropout layers to lessen the effect of overfitting\n",
    "- Monitored the model on perplexity rather than accuracy during the training\n",
    "- Used the test corpus as the validation set during the training to evaluate how well the model generalize at each epoch\n",
    "\n",
    "Since the validation perplexity did not improve within 10 epochs, the training had an early stop at epoch # 12, which resulted in a **perplexity of 78.30 on the training set**. Had I let the model to continue training until 100 epochs, the model will certainly reach a much lower perplexity on the training set, but it will also widen the gap between the training perplexity and the validation perplexity (i.e. the model will not generalize well on the test set). \n",
    "\n",
    "The final model evaluated on the evaluation set gives **a perplexity of 2192.34** which is still a pretty large value. The large gap between the training perplexity and the test perplexity likely implied that there were overfitting occuring. This could mean that the training corpus does not represent the evaluation corpus well in its current state. Some factors could be contributing to that:\n",
    "\n",
    "- Amount of OOV in the evaluation corpus\n",
    "- Change of writing style (Could a 10 years gap between the release of \"The Adventures of Sherlock Holmes\" and \"The Hound of the Baskervilles\" possibly changed the writing style of Sir Arthur Conan Doyle?)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

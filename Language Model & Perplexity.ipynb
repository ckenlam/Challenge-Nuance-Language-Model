{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the \"hound-train.txt\" provided with this challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hound-train.txt', encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " \"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\\n\",\n",
       " '\\n',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with\\n',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or\\n',\n",
       " 're-use it under the terms of the Project Gutenberg License included\\n',\n",
       " 'with this eBook or online at www.gutenberg.net\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Title: The Adventures of Sherlock Holmes\\n',\n",
       " '\\n',\n",
       " 'Author: Arthur Conan Doyle\\n',\n",
       " '\\n',\n",
       " 'Release Date: November 29, 2002 [EBook #1661]\\n',\n",
       " 'Last Updated: May 20, 2019\\n',\n",
       " '\\n',\n",
       " 'Language: English\\n',\n",
       " '\\n',\n",
       " 'Character set encoding: UTF-8\\n',\n",
       " '\\n',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Produced by an anonymous Project Gutenberg volunteer and Jose Menendez\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'cover\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The Adventures of Sherlock Holmes\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'by Arthur Conan Doyle\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Contents\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   I.     A Scandal in Bohemia\\n',\n",
       " '   II.    The Red-Headed League\\n',\n",
       " '   III.   A Case of Identity\\n',\n",
       " '   IV.    The Boscombe Valley Mystery\\n',\n",
       " '   V.     The Five Orange Pips\\n',\n",
       " '   VI.    The Man with the Twisted Lip\\n',\n",
       " '   VII.   The Adventure of the Blue Carbuncle\\n',\n",
       " '   VIII.  The Adventure of the Speckled Band\\n',\n",
       " '   IX.    The Adventure of the Engineer’s Thumb\\n',\n",
       " '   X.     The Adventure of the Noble Bachelor\\n',\n",
       " '   XI.    The Adventure of the Beryl Coronet\\n',\n",
       " '   XII.   The Adventure of the Copper Beeches\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'I. A SCANDAL IN BOHEMIA\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'I.\\n',\n",
       " '\\n',\n",
       " 'To Sherlock Holmes she is always _the_ woman. I have seldom heard him\\n',\n",
       " 'mention her under any other name. In his eyes she eclipses and\\n',\n",
       " 'predominates the whole of her sex. It was not that he felt any emotion\\n',\n",
       " 'akin to love for Irene Adler. All emotions, and that one particularly,\\n',\n",
       " 'were abhorrent to his cold, precise but admirably balanced mind. He\\n',\n",
       " 'was, I take it, the most perfect reasoning and observing machine that\\n',\n",
       " 'the world has seen, but as a lover he would have placed himself in a\\n',\n",
       " 'false position. He never spoke of the softer passions, save with a gibe\\n',\n",
       " 'and a sneer. They were admirable things for the observer—excellent for\\n',\n",
       " 'drawing the veil from men’s motives and actions. But for the trained\\n',\n",
       " 'reasoner to admit such intrusions into his own delicate and finely\\n',\n",
       " 'adjusted temperament was to introduce a distracting factor which might\\n',\n",
       " 'throw a doubt upon all his mental results. Grit in a sensitive\\n',\n",
       " 'instrument, or a crack in one of his own high-power lenses, would not\\n',\n",
       " 'be more disturbing than a strong emotion in a nature such as his. And\\n',\n",
       " 'yet there was but one woman to him, and that woman was the late Irene\\n',\n",
       " 'Adler, of dubious and questionable memory.\\n',\n",
       " '\\n',\n",
       " 'I had seen little of Holmes lately. My marriage had drifted us away\\n',\n",
       " 'from each other. My own complete happiness, and the home-centred\\n',\n",
       " 'interests which rise up around the man who first finds himself master\\n',\n",
       " 'of his own establishment, were sufficient to absorb all my attention,\\n',\n",
       " 'while Holmes, who loathed every form of society with his whole Bohemian\\n',\n",
       " 'soul, remained in our lodgings in Baker Street, buried among his old\\n',\n",
       " 'books, and alternating from week to week between cocaine and ambition,\\n',\n",
       " 'the drowsiness of the drug, and the fierce energy of his own keen\\n',\n",
       " 'nature. He was still, as ever, deeply attracted by the study of crime,\\n',\n",
       " 'and occupied his immense faculties and extraordinary powers of\\n',\n",
       " 'observation in following out those clues, and clearing up those\\n',\n",
       " 'mysteries which had been abandoned as hopeless by the official police.\\n',\n",
       " 'From time to time I heard some vague account of his doings: of his\\n',\n",
       " 'summons to Odessa in the case of the Trepoff murder, of his clearing up\\n',\n",
       " 'of the singular tragedy of the Atkinson brothers at Trincomalee, and\\n',\n",
       " 'finally of the mission which he had accomplished so delicately and\\n',\n",
       " 'successfully for the reigning family of Holland. Beyond these signs of\\n',\n",
       " 'his activity, however, which I merely shared with all the readers of\\n',\n",
       " 'the daily press, I knew little of my former friend and companion.\\n']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will only be using the content from the dataset that are relevant to the training of the language model by finding the starting and ending indexes containing the first sentence and the last sentence of the book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "startString = 'To Sherlock Holmes she is always'\n",
    "startIndex = [i for i, s in enumerate(data) if startString in s][0]\n",
    "endString = 'Walsall, where I believe that she has met with considerable success.'\n",
    "endtIndex = [i for i, s in enumerate(data) if endString in s][0]\n",
    "text_data = ' '.join(data[startIndex:endtIndex+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To Sherlock Holmes she is always _the_ woman. I have seldom heard him\\n mention her under any other name. In his eyes she eclipses and\\n predominates the whole of her sex. It was not that he felt any emotion\\n akin to love for Irene Adler. All emotions, and that one particularly,\\n were abhorrent to his cold, precise but admirably balanced mind. He\\n was, I take it, the most perfect reasoning and observing machine that\\n the world has seen, but as a lover he would have placed himself in a\\n false position. He never spoke of the softer passions, save with a gibe\\n and a sneer. They were admirable things for the observer—excellent for\\n drawing the veil from men’s motives and actions. But for the trained\\n reasoner to admit such intrusions into his own delicate and finely\\n adjusted temperament was to introduce a distracting factor which might\\n throw a doubt upon all his mental results. Grit in a sensitive\\n instrument, or a crack in one of his own high-power lenses, would not\\n be more disturbing than a strong emotion in a nature such as his. And\\n yet there was but one woman to him, and that woman was the late Irene\\n Adler, of dubious and questionable memory.\\n \\n I had seen little of Holmes lately. My marriage had drifted us away\\n from each other. My own complete happiness, and the home-centred\\n interests which rise up around the man who first finds himself master\\n of his own establishment, were sufficient to absorb all my attention,\\n while Holmes, who loathed every form of society with his whole Bohemian\\n soul, remained in our lodgings in Baker Street, buried among his old\\n books, and alternating from week to week between cocaine and ambition,\\n the drowsiness of the drug, and the fierce energy of his own keen\\n nature. He was still, as ever, deeply attracted by the study of crime,\\n and occupied his immense faculties and extraordinary powers of\\n observation in following out those clues, and clearing up those\\n mysteries which had been abandoned as hopeless by the official police.\\n From time to time I heard some vague account of his doings: of his\\n summons to Odessa in the case of the Trepoff murder, of his clearing up\\n of the singular tragedy of the Atkinson brothers at Trincomalee, and\\n finally of the mission which he had accomplished so delicately and\\n successfully for the reigning family of Holland. Beyond these signs of\\n his activity, however, which I merely shared with all the readers of\\n the daily press, I knew little of my former friend and companion.\\n \\n One night—it was on the twentieth of March, 1888—I was returning from a\\n journey to a patient (for I had now returned to civil practice), when\\n my way led me through Baker Street. As I passed the well-remembered\\n door, which must always be associated in my mind with my wooing, and\\n with the dark incidents of the Study in Scarlet, I was seized with a\\n keen desire to see Holmes again, and to know how he was employing his\\n extraordinary powers. His rooms were brilliantly lit, and, even as I\\n looked up, I saw his tall, spare figure pass twice in a dark silhouette\\n against the blind. He was pacing the room swiftly, eagerly, with his\\n head sunk upon his chest and his hands clasped behind him. To me, who\\n knew his every mood and habit, his attitude and manner told their own\\n story. He was at work again. He had risen out of his drug-created\\n dreams and was hot upon the scent of some new problem. I rang the bell\\n and was shown up to the chamber which had formerly been in part my own.\\n \\n His manner was not effusive. It seldom was; but he was glad, I think,\\n to see me. With hardly a word spoken, but with a kindly eye, he waved\\n me to an armchair, threw across his case of cigars, and indicated a\\n spirit case and a gasogene in the corner. Then he stood before the fire\\n and looked me over in his singular introspective fashion.\\n \\n “Wedlock suits you,” he remarked. “I think, Watson, that you have put\\n on seven and a half pounds since I saw you.”\\n \\n “Seven!” I answered.\\n \\n “Indeed, I should have thought a little more. Just a trifle more, I\\n fancy, Watson. And in practice again, I observe. You did not tell me\\n that you intended to go into harness.”\\n \\n “Then, how do you know?”\\n \\n “I see it, I deduce it. How do I know that you have been getting\\n yourself very wet lately, and that you have a most clumsy and careless\\n servant girl?”\\n \\n “My dear Holmes,” said I, “this is too much. You would certainly have\\n been burned, had you lived a few centuries ago. It is true that I had a\\n country walk on Thursday and came home in a dreadful mess, but as I\\n have changed my clothes I can’t imagine how you deduce it. As to Mary\\n Jane, she is incorrigible, and my wife has given her notice, but there,\\n again, I fail to see how you work it out.”\\n \\n He chuckled to himself and rubbed his long, nervous hands together.\\n \\n “It is simplicity itself,” said he; “my eyes tell me that on the inside\\n of your left shoe, just where the firelight strikes it, the leather is\\n scored by six almost parallel cuts. Obviously '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate realistic sentences, I need to preserve the punctuations in the dataset. However, Keras' Tokenizer will remove all punctuations by default. To get around that, I will have to add a space to any punctuations and remove the desired punctuations from the Tokenizer's filter. I will also capitalize all letters to match what is in the test set. \n",
    "\n",
    "I will then organize the tokens into sequences of 50 input words and 1 output word. That is, sequences of 51 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', oov_token='OOV', lower=False)\n",
    "\n",
    "def data_processing(text_data, seq_length=50):\n",
    "    #Capitalize all letters\n",
    "    text = text_data.upper()\n",
    "    #Isolate punctuations; except the periods\n",
    "    cleanup_dict = {\",\":\" ,\"\n",
    "               ,\"!\":\" !\"\n",
    "               ,\"?\":\" ?\"\n",
    "               ,\"\\n\":\"\"\n",
    "               ,\"_\":\"\"\n",
    "               ,\"“\":\"\"\n",
    "               ,\"”\":\"\"\n",
    "               ,\"(\":\"( \"\n",
    "               ,\")\":\" )\"\n",
    "               ,\"II.\":\"\"\n",
    "               ,\"III.\":\"\"\n",
    "               ,\"IV.\":\"\"\n",
    "               ,\"V.\":\"\"\n",
    "               ,\"VI.\":\"\" \n",
    "               ,\"VII.\":\"\"\n",
    "               ,\"VIII.\":\"\" \n",
    "               ,\"IX.\":\"\" \n",
    "               ,\"X.\":\"\"\n",
    "               ,\"XI.\":\"\" \n",
    "               ,\"XII.\":\"\"                    \n",
    "                }\n",
    "    for from_this, to_this in cleanup_dict.items():\n",
    "        text = text.replace(from_this, to_this)\n",
    "    #I want to preserve periods that designate accronyms (ones that are not followed by any space)\n",
    "    text = re.sub('\\.\\s+',' . ', text)\n",
    "    #Tokenize all words in the text\n",
    "    text = text_to_word_sequence(text, filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', lower=False)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    tokens = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    words_count = len(word_index)+1\n",
    "    #Turn the tokenized text into sequences of the specified length and append to the \"sequences\" list\n",
    "    sequences = []\n",
    "    length = seq_length +1\n",
    "    #For the first 50 words\n",
    "    for i in range(1, length):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[0:i+1]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "    #For the rest of the data \n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "        \n",
    "    #Finding the maximum length in this dataset (just in case...)\n",
    "    max_sequence_length = max(len(x) for x in sequences)    \n",
    "    #Make sure all sequences are of the same length\n",
    "    sequences = pad_sequences(sequences,maxlen=max_sequence_length,padding='pre')\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "    y = ku.to_categorical(y, num_classes=words_count)\n",
    "\n",
    "    return X,y,max_sequence_length,words_count, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, max_seq_length, total_words_count, word_index = data_processing(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer since we will need it for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, I will train a neural network with two LSTM hidden layers with 100 neurons each. A dense fully connected layer with 100 neurons connects to the LSTM hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 10)            81910     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50, 100)           44400     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8191)              827291    \n",
      "=================================================================\n",
      "Total params: 1,044,101\n",
      "Trainable params: 1,044,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words_count, 10, input_length=max_seq_length - 1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(total_words_count, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History \n",
    "from keras.callbacks import EarlyStopping\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96363 samples, validate on 24091 samples\n",
      "Epoch 1/50\n",
      "96363/96363 [==============================] - 205s 2ms/step - loss: 6.2674 - accuracy: 0.0636 - val_loss: 6.0147 - val_accuracy: 0.0699\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ken.lam\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37_env\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with  val_accuracy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96363/96363 [==============================] - 194s 2ms/step - loss: 5.7975 - accuracy: 0.0877 - val_loss: 5.8733 - val_accuracy: 0.0884\n",
      "Epoch 3/50\n",
      "96363/96363 [==============================] - 192s 2ms/step - loss: 5.5554 - accuracy: 0.1061 - val_loss: 5.7609 - val_accuracy: 0.1066\n",
      "Epoch 4/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 5.3812 - accuracy: 0.1189 - val_loss: 5.6969 - val_accuracy: 0.1175\n",
      "Epoch 5/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 5.2257 - accuracy: 0.1324 - val_loss: 5.6707 - val_accuracy: 0.1308\n",
      "Epoch 6/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 5.1054 - accuracy: 0.1416 - val_loss: 5.6668 - val_accuracy: 0.1348\n",
      "Epoch 7/50\n",
      "96363/96363 [==============================] - 197s 2ms/step - loss: 5.0045 - accuracy: 0.1486 - val_loss: 5.6588 - val_accuracy: 0.1390\n",
      "Epoch 8/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.9113 - accuracy: 0.1549 - val_loss: 5.6856 - val_accuracy: 0.1400\n",
      "Epoch 9/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.8243 - accuracy: 0.1598 - val_loss: 5.6892 - val_accuracy: 0.1440\n",
      "Epoch 10/50\n",
      "96363/96363 [==============================] - 192s 2ms/step - loss: 4.7399 - accuracy: 0.1641 - val_loss: 5.7221 - val_accuracy: 0.1469\n",
      "Epoch 11/50\n",
      "96363/96363 [==============================] - 200s 2ms/step - loss: 4.6576 - accuracy: 0.1700 - val_loss: 5.7497 - val_accuracy: 0.1506\n",
      "Epoch 12/50\n",
      "96363/96363 [==============================] - 206s 2ms/step - loss: 4.5793 - accuracy: 0.1743 - val_loss: 5.8310 - val_accuracy: 0.1516\n",
      "Epoch 13/50\n",
      "96363/96363 [==============================] - 215s 2ms/step - loss: 4.5054 - accuracy: 0.1786 - val_loss: 5.8831 - val_accuracy: 0.1527\n",
      "Epoch 14/50\n",
      "96363/96363 [==============================] - 212s 2ms/step - loss: 4.4363 - accuracy: 0.1810 - val_loss: 5.9801 - val_accuracy: 0.1551\n",
      "Epoch 15/50\n",
      "96363/96363 [==============================] - 195s 2ms/step - loss: 4.3647 - accuracy: 0.1848 - val_loss: 6.0661 - val_accuracy: 0.1555\n",
      "Epoch 16/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 4.4880 - accuracy: 0.1751 - val_loss: 6.2379 - val_accuracy: 0.1495\n",
      "Epoch 17/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 4.3779 - accuracy: 0.1803 - val_loss: 6.2900 - val_accuracy: 0.1519\n",
      "Epoch 18/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.2953 - accuracy: 0.1837 - val_loss: 6.4009 - val_accuracy: 0.1545\n",
      "Epoch 19/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.2444 - accuracy: 0.1868 - val_loss: 6.5304 - val_accuracy: 0.1544\n",
      "Epoch 20/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.1708 - accuracy: 0.1911 - val_loss: 6.6122 - val_accuracy: 0.1567\n",
      "Epoch 21/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 4.1172 - accuracy: 0.1935 - val_loss: 6.7947 - val_accuracy: 0.1549\n",
      "Epoch 22/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 4.0539 - accuracy: 0.1972 - val_loss: 6.9270 - val_accuracy: 0.1579\n",
      "Epoch 23/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 3.9952 - accuracy: 0.2021 - val_loss: 7.0890 - val_accuracy: 0.1558\n",
      "Epoch 24/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 3.9378 - accuracy: 0.2062 - val_loss: 7.2299 - val_accuracy: 0.1573\n",
      "Epoch 25/50\n",
      "96363/96363 [==============================] - 195s 2ms/step - loss: 3.8801 - accuracy: 0.2100 - val_loss: 7.3242 - val_accuracy: 0.1576\n",
      "Epoch 26/50\n",
      "96363/96363 [==============================] - 193s 2ms/step - loss: 3.8260 - accuracy: 0.2154 - val_loss: 7.5468 - val_accuracy: 0.1560\n",
      "Epoch 27/50\n",
      "96363/96363 [==============================] - 192s 2ms/step - loss: 3.7740 - accuracy: 0.2194 - val_loss: 7.6209 - val_accuracy: 0.1574\n",
      "Epoch 28/50\n",
      "96363/96363 [==============================] - 194s 2ms/step - loss: 3.7200 - accuracy: 0.2235 - val_loss: 7.7629 - val_accuracy: 0.1551\n",
      "Epoch 29/50\n",
      "96363/96363 [==============================] - 204s 2ms/step - loss: 3.6728 - accuracy: 0.2280 - val_loss: 7.9497 - val_accuracy: 0.1579\n",
      "Epoch 30/50\n",
      "96363/96363 [==============================] - 204s 2ms/step - loss: 3.6233 - accuracy: 0.2327 - val_loss: 8.0684 - val_accuracy: 0.1511\n",
      "Epoch 31/50\n",
      "96363/96363 [==============================] - 201s 2ms/step - loss: 3.5782 - accuracy: 0.2372 - val_loss: 8.2107 - val_accuracy: 0.1523\n",
      "Epoch 32/50\n",
      "96363/96363 [==============================] - 209s 2ms/step - loss: 3.5355 - accuracy: 0.2416 - val_loss: 8.4224 - val_accuracy: 0.1513\n",
      "Epoch 33/50\n",
      "96363/96363 [==============================] - 200s 2ms/step - loss: 3.4927 - accuracy: 0.2455 - val_loss: 8.4998 - val_accuracy: 0.1515\n",
      "Epoch 34/50\n",
      "96363/96363 [==============================] - 201s 2ms/step - loss: 3.4500 - accuracy: 0.2496 - val_loss: 8.6313 - val_accuracy: 0.1530\n",
      "Epoch 35/50\n",
      "96363/96363 [==============================] - 204s 2ms/step - loss: 3.4096 - accuracy: 0.2562 - val_loss: 8.8562 - val_accuracy: 0.1536\n",
      "Epoch 36/50\n",
      "96363/96363 [==============================] - 206s 2ms/step - loss: 3.3740 - accuracy: 0.2603 - val_loss: 8.8902 - val_accuracy: 0.1491\n",
      "Epoch 37/50\n",
      "96363/96363 [==============================] - 204s 2ms/step - loss: 3.3351 - accuracy: 0.2648 - val_loss: 9.0620 - val_accuracy: 0.1510\n",
      "Epoch 38/50\n",
      "96363/96363 [==============================] - 206s 2ms/step - loss: 3.2984 - accuracy: 0.2692 - val_loss: 9.2595 - val_accuracy: 0.1497\n",
      "Epoch 39/50\n",
      "96363/96363 [==============================] - 198s 2ms/step - loss: 3.2636 - accuracy: 0.2738 - val_loss: 9.3460 - val_accuracy: 0.1483\n",
      "Epoch 40/50\n",
      "96363/96363 [==============================] - 196s 2ms/step - loss: 3.2293 - accuracy: 0.2797 - val_loss: 9.4732 - val_accuracy: 0.1484\n",
      "Epoch 41/50\n",
      "96363/96363 [==============================] - 196s 2ms/step - loss: 3.1965 - accuracy: 0.2832 - val_loss: 9.6246 - val_accuracy: 0.1476\n",
      "Epoch 42/50\n",
      "96363/96363 [==============================] - 196s 2ms/step - loss: 3.1650 - accuracy: 0.2872 - val_loss: 9.7552 - val_accuracy: 0.1455\n",
      "Epoch 43/50\n",
      "96363/96363 [==============================] - 198s 2ms/step - loss: 3.1341 - accuracy: 0.2921 - val_loss: 9.8267 - val_accuracy: 0.1495\n",
      "Epoch 44/50\n",
      "96363/96363 [==============================] - 198s 2ms/step - loss: 3.1028 - accuracy: 0.2974 - val_loss: 9.9639 - val_accuracy: 0.1469\n",
      "Epoch 45/50\n",
      "96363/96363 [==============================] - 197s 2ms/step - loss: 3.0755 - accuracy: 0.3002 - val_loss: 10.1249 - val_accuracy: 0.1433\n",
      "Epoch 46/50\n",
      "96363/96363 [==============================] - 198s 2ms/step - loss: 3.0441 - accuracy: 0.3049 - val_loss: 10.2179 - val_accuracy: 0.1454\n",
      "Epoch 47/50\n",
      "96363/96363 [==============================] - 199s 2ms/step - loss: 3.0159 - accuracy: 0.3100 - val_loss: 10.3375 - val_accuracy: 0.1475\n",
      "Epoch 48/50\n",
      "96363/96363 [==============================] - 199s 2ms/step - loss: 2.9893 - accuracy: 0.3135 - val_loss: 10.4333 - val_accuracy: 0.1436\n",
      "Epoch 49/50\n",
      "96363/96363 [==============================] - 199s 2ms/step - loss: 2.9617 - accuracy: 0.3191 - val_loss: 10.5525 - val_accuracy: 0.1418\n",
      "Epoch 50/50\n",
      "96363/96363 [==============================] - 210s 2ms/step - loss: 2.9345 - accuracy: 0.3230 - val_loss: 10.6802 - val_accuracy: 0.1428\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=' val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(X, y, epochs=epochs, verbose = 1\n",
    "                      , batch_size=batch_size\n",
    "                      , callbacks=callbacks_list\n",
    "                      ,validation_split=0.2\n",
    "                      #,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "                     )\n",
    "print(\"Training completed!\")\n",
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5',compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hound-test.txt', encoding=\"utf-8\") as f:\n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that process the test set into the format that the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_processing(text_data, tokenizer, seq_length=50):\n",
    "    text = text_data \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    #Tokenize all words in the text\n",
    "    text = text_to_word_sequence(text, filters='‘’\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t\\n', lower=False)\n",
    "    tokens = tokenizer.texts_to_sequences(text)\n",
    "    #Turn the tokenized text into sequences of the specified length and append to the \"sequences\" list\n",
    "    sequences = []\n",
    "    length = seq_length +1\n",
    "    \n",
    "    #For the first 50 words\n",
    "    for i in range(1, length):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[0:i+1]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "    #For the rest of the data \n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        flattened_seq = [val for sublist in seq for val in sublist]\n",
    "        sequences.append(flattened_seq)\n",
    "        \n",
    "    #Finding the maximum length in this dataset (just in case...)\n",
    "    max_sequence_length = max(len(x) for x in sequences)    \n",
    "    #Make sure all sequences are of the same length\n",
    "    sequences = pad_sequences(sequences,maxlen=51,padding='pre')\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest, ytest = test_data_processing(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions from the test data\n",
    "ypred=model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the probabilities assigned to the correct words from the test sequences\n",
    "def model_perplexity(ytest,ypred, dictionary, verbose=False):\n",
    "    y_test_proba = []\n",
    "    \n",
    "    for i in tqdm(range(ytest.shape[0])):\n",
    "        if verbose:\n",
    "            print(f\"Probability of the word {[key for key in dictionary.items() if key[1] == ytest[i]][0][0]} is {ypred[i][ytest[i]]}.\")\n",
    "        y_test_proba.append(ypred[i][ytest[i]])\n",
    "    return y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 66729/66729 [00:02<00:00, 30438.45it/s]\n"
     ]
    }
   ],
   "source": [
    "ytest_probability =  model_perplexity(ytest,ypred, word_index)\n",
    "ytest_probability = np.asarray(ytest_probability)[np.nonzero(np.asarray(ytest_probability))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is a measurement of how well a probability model predicts a test data and is the exponentiation of the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47472.87709264595\n"
     ]
    }
   ],
   "source": [
    "# get the entropy and perplexity\n",
    "entropy=np.mean(-np.log(ytest_probability))\n",
    "print(math.exp(entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My time was mainly spent on comparing the training set to the test set and coming up with data processing steps that would make the training set to ressemble the test set. The following is list of the data processing steps:\n",
    "- Capitalize all letters\n",
    "- Add space on both sides of the punctuation such as \"!\", \"?\", \",\", and \".\" (except for accronyms) to mimic the test set\n",
    "- Remove any punctuations that do not exist in the test set\n",
    "- Removing irrelevant content from the training set\n",
    "- Transform the training set into sequences of 51 words as input data to the LSTM Language Model\n",
    "\n",
    "The calculated perplexity on this model is 47472.88. Obviously, this LSTM model that was trained on 50 epochs only is a poor language model since the perplexity is **extremely high**. \n",
    "\n",
    "Given the time, I would improve the results by doing the following: \n",
    "- Start with a simpler n-gram model with a proper smoothing to be used as a benchmark \n",
    "- Train the model on more epochs with the same architecture\n",
    "- Increase the complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
